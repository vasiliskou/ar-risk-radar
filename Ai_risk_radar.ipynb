{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß≠ AI-Risk Radar: Monitoring Global AI Developments with LLMs & Clustering\n",
        "\n",
        "This interactive notebook tracks emerging AI-related risks and policy trends by:\n",
        "\n",
        "- Scraping real-time news from trusted technology and policy sources  \n",
        "- Embedding and clustering articles using semantic similarity  \n",
        "- Labeling topics with OpenAI's `gpt-4o-mini` model  \n",
        "- Generating concise cluster summaries via LLMs  \n",
        "- Presenting everything in an intuitive Gradio dashboard\n",
        "\n",
        "Use this tool to explore how AI is shaping geopolitics, energy infrastructure, ethics, and regulation ‚Äî all in one place.\n"
      ],
      "metadata": {
        "id": "p_O8FazykTnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Install Required Libraries for AI Risk Radar Pipeline\n"
      ],
      "metadata": {
        "id": "cWvK4rzV0l6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai newspaper3k feedparser \\\n",
        "             sentence-transformers chromadb umap-learn hdbscan \\\n",
        "             plotly dash langchain-community lxml-html-clean gradio"
      ],
      "metadata": {
        "id": "EZsUQEwQz7wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Import Standard Libraries and External Modules\n"
      ],
      "metadata": {
        "id": "WE2fB_VbadZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os, datetime, json, pathlib, textwrap, tempfile, itertools\n",
        "from typing import List, Dict, Any  # For type annotations\n",
        "\n",
        "# News feed parsing and article extraction\n",
        "import feedparser         # Parse RSS/Atom feeds\n",
        "import newspaper          # Scrape full-text news articles\n",
        "\n",
        "# Data manipulation and progress bars\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm  # Smart progress bar for loops\n",
        "\n",
        "# Embeddings and vector database\n",
        "from sentence_transformers import SentenceTransformer  # To embed article text\n",
        "import chromadb                                          # Vector DB for similarity search\n",
        "from chromadb.utils import embedding_functions           # (Optional utility functions)\n",
        "\n",
        "# Dimensionality reduction and clustering\n",
        "import umap     # For 2D projection of embeddings\n",
        "import hdbscan  # Hierarchical density-based clustering\n",
        "from sklearn.cluster import KMeans # Clustering method\n",
        "\n",
        "# OpenAI API for summaries and labeling\n",
        "import openai\n",
        "\n",
        "# Gradio UI framework for building the interactive app\n",
        "import gradio as gr\n",
        "\n",
        "# Dashboard components (was used for Dash-based UI)\n",
        "import plotly.express as px\n",
        "from dash import Dash, html, dcc, dash_table, Input, Output\n",
        "\n",
        "# üåê Google Colab utility for securely loading API keys\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "gvp8lW780HIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚öôÔ∏è Runtime Configuration and API Setup\n"
      ],
      "metadata": {
        "id": "6XBzC7rT0LLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime settings\n",
        "BATCH_SIZE = 8  # Number of articles to process in one embedding batch\n",
        "EMBED_MODEL_NAME = \"intfloat/e5-large-v2\"  # SentenceTransformer model for multilingual embeddings\n",
        "VECTOR_DB_DIR = \"./chroma_db\"  # Directory for persistent Chroma vector database\n",
        "\n",
        "# News feeds to monitor\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.euronews.com/tag/artificial-intelligence/rss\",  # Euronews AI section\n",
        "    \"https://www.brookings.edu/topic/artificial-intelligence/feed/\",  # Brookings Institution\n",
        "    \"https://news.un.org/feed/subscribe/en/news/topic/technology/feed/rss.xml\",  # UN tech news\n",
        "    \"https://www.technologyreview.com/feed/\",  # MIT Tech Review\n",
        "    \"https://feeds.arstechnica.com/arstechnica/technology-lab\",  # Ars Technica tech lab\n",
        "]\n",
        "\n",
        "# OpenAI API setup via Google Colab secure storage\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")  # Securely load OpenAI key from Colab userdata\n",
        "if openai.api_key is None:\n",
        "    raise ValueError(\"‚ùå OPENAI_API_KEY not found in userdata. Please set it using: userdata.set('OPENAI_API_KEY', 'sk-...')\")\n",
        "print(\"‚úÖ OpenAI API key loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwctAjzV0Piz",
        "outputId": "c83a569c-2565-4724-926b-ce236b2e7b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì° AI-Risk Radar: Multi-Agent Pipeline Definition\n",
        "\n",
        "This cell defines all core components of the AI-Risk Radar system. It includes modular agents that:\n",
        "\n",
        "- üîç **Scrape** news from various AI-related RSS feeds  \n",
        "- üß† **Embed** articles using sentence-transformers (stored in ChromaDB for persistence)  \n",
        "- üß≠ **Cluster** similar articles using UMAP + HDBSCAN or KMeans  \n",
        "- üè∑Ô∏è **Label** each cluster with a concise topic using an LLM (`gpt-4o-mini`)  \n",
        "- üìù **Summarize** each cluster‚Äôs key themes via OpenAI‚Äôs GPT API  \n",
        "- üíª **Visualize** everything in a Gradio-powered interactive dashboard\n",
        "\n",
        "The system provides a high-level overview of AI-related global developments, with options to download per-cluster data or generate a weekly summary using LLM analysis.\n"
      ],
      "metadata": {
        "id": "YGldnHJ10Rg7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJccL2V1yEsz"
      },
      "outputs": [],
      "source": [
        "class NewsScraperAgent:\n",
        "    \"\"\"Fetch articles from RSS feeds and build a cleaned DataFrame.\"\"\"\n",
        "\n",
        "    def __init__(self, feeds: List[str]):\n",
        "        self.feeds = feeds\n",
        "\n",
        "    def run(self) -> pd.DataFrame:\n",
        "        records = []\n",
        "        for url in self.feeds:\n",
        "            print(f\"üì° Parsing feed: {url}\")\n",
        "            fp = feedparser.parse(url)  # Parse RSS feed\n",
        "            print(f\"  ‚Üí Found {len(fp.entries)} entries\")\n",
        "            for entry in fp.entries:\n",
        "                records.append({\n",
        "                    \"title\": entry.get(\"title\", \"\"),\n",
        "                    \"link\": entry.get(\"link\", \"\"),\n",
        "                    \"published\": entry.get(\"published\", \"\"),\n",
        "                    \"summary\": entry.get(\"summary\", \"\"),\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(records)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"‚ö†Ô∏è No entries found in feeds.\")\n",
        "            return df\n",
        "\n",
        "        # Download full articles using newspaper3k\n",
        "        texts = []\n",
        "        for link in tqdm(df[\"link\"], desc=\"üì• Downloading articles\"):\n",
        "            try:\n",
        "                a = newspaper.Article(link, language=\"en\")\n",
        "                a.download()\n",
        "                a.parse()\n",
        "                texts.append(a.text)\n",
        "            except Exception:\n",
        "                texts.append(\"\")\n",
        "\n",
        "        df[\"text\"] = texts\n",
        "        df[\"published_dt\"] = pd.to_datetime(df.published, errors=\"coerce\")\n",
        "\n",
        "        # Filter: remove short, empty, or paywalled articles\n",
        "        def is_valid_article(text):\n",
        "            text = text.lower().strip()\n",
        "            return (\n",
        "                len(text) > 300 and\n",
        "                \"try unlimited access\" not in text and\n",
        "                \"subscribe\" not in text and\n",
        "                \"join ft edit\" not in text\n",
        "            )\n",
        "\n",
        "        df = df[df[\"text\"].apply(is_valid_article)].reset_index(drop=True)\n",
        "        print(f\"‚úÖ Retained {len(df)} valid articles after filtering\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "class EmbeddingAgent:\n",
        "    \"\"\"Embed articles using SentenceTransformer and store them in Chroma vector DB.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, persist_dir: str):\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        import chromadb\n",
        "\n",
        "        self.embedder = SentenceTransformer(model_name)  # Load embedding model\n",
        "        self.client = chromadb.PersistentClient(path=persist_dir)  # Set up persistent DB\n",
        "\n",
        "        try:\n",
        "            self.client.delete_collection(\"articles\")  # Reset collection if it exists\n",
        "            print(\"üßº Deleted existing 'articles' collection to reset it\")\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Could not delete existing collection:\", e)\n",
        "\n",
        "        self.collection = self.client.get_or_create_collection(\"articles\")\n",
        "\n",
        "    def run(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        texts = []\n",
        "        ids = []\n",
        "        metadatas = []\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            if not row.text or not row.text.strip():\n",
        "                continue\n",
        "            metadata = {\n",
        "                k: (str(v) if isinstance(v, (pd.Timestamp, datetime.datetime)) else v)\n",
        "                for k, v in row.to_dict().items()\n",
        "            }\n",
        "            texts.append(row.text)\n",
        "            ids.append(str(i))\n",
        "            metadatas.append(metadata)\n",
        "\n",
        "        if not texts:\n",
        "            raise ValueError(\"‚ùå No valid texts found for embedding.\")\n",
        "\n",
        "        print(f\"üìÑ Generating embeddings for {len(texts)} documents...\")\n",
        "        vectors = self.embedder.encode(texts, show_progress_bar=True).tolist()\n",
        "\n",
        "        self.collection.add(\n",
        "            ids=ids,\n",
        "            documents=texts,\n",
        "            metadatas=metadatas,\n",
        "            embeddings=vectors\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Added {len(vectors)} embeddings to ChromaDB\")\n",
        "\n",
        "        return pd.DataFrame(vectors)\n",
        "\n",
        "\n",
        "class ClusteringAgent:\n",
        "    \"\"\"Cluster and label articles using unsupervised ML and LLM-based labeling.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass  # Could configure LLM settings here if needed\n",
        "\n",
        "    def label_cluster_with_llm(self, texts: List[str], model=\"gpt-4o-mini\") -> str:\n",
        "        # Use OpenAI LLM to assign a single label for the cluster\n",
        "        try:\n",
        "            joined = \"\\n\\n\".join(texts)[:8000]\n",
        "            response = openai.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert in analyzing clusters of geopolitical news.\"},\n",
        "                    {\"role\": \"user\", \"content\": (\n",
        "                        \"You are an expert policy analyst. \"\n",
        "                        \"Given the following articles in a single topic cluster, return ONLY ONE short, clear, and descriptive topic label \"\n",
        "                        \"(5 words or fewer). Do NOT include bullet points or markdown. Just return the label text.\\n\\n\"\n",
        "                        f\"{joined}\"\n",
        "                    )}\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "            )\n",
        "            label = response.choices[0].message.content.strip()\n",
        "            return label\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è LLM labelling failed: {e}\")\n",
        "            return \"Unlabelled\"\n",
        "\n",
        "    def run(self, embeddings_df: pd.DataFrame, df: pd.DataFrame) -> tuple:\n",
        "        import umap\n",
        "        import hdbscan\n",
        "\n",
        "        # Reduce dimensionality for clustering and plotting\n",
        "        reducer = umap.UMAP(n_neighbors=5, min_dist=0.0, metric=\"cosine\")\n",
        "        embedding_2d = reducer.fit_transform(embeddings_df.values)\n",
        "\n",
        "        # Apply HDBSCAN clustering\n",
        "        clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric=\"euclidean\")\n",
        "        labels = clusterer.fit_predict(embedding_2d)\n",
        "\n",
        "        if len(set(labels)) <= 1 or all(label == -1 for label in labels):\n",
        "            print(\"‚ö†Ô∏è HDBSCAN found no valid clusters ‚Äî falling back to KMeans.\")\n",
        "            kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "            labels = kmeans.fit_predict(embedding_2d)\n",
        "\n",
        "        df_clustered = df.copy()\n",
        "        df_clustered[\"cluster\"] = labels\n",
        "\n",
        "        # Label each cluster using LLM\n",
        "        cluster_labels = {}\n",
        "        for c in set(labels):\n",
        "            texts = df_clustered[df_clustered.cluster == c].text.tolist()\n",
        "            label = self.label_cluster_with_llm(texts)\n",
        "            cluster_labels[c] = label\n",
        "\n",
        "        df_clustered[\"topic\"] = df_clustered[\"cluster\"].map(cluster_labels)\n",
        "\n",
        "        return df_clustered, embedding_2d, cluster_labels\n",
        "\n",
        "\n",
        "class SummariserAgent:\n",
        "    \"\"\"Summarise each cluster into a brief using OpenAI.\"\"\"\n",
        "\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\"):\n",
        "        import openai\n",
        "        from google.colab import userdata\n",
        "\n",
        "        self.openai = openai\n",
        "        self.model = model\n",
        "        self.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"‚ùå OPENAI_API_KEY not set in Colab. Use `userdata.set(...)` to define it.\")\n",
        "        self.openai.api_key = self.api_key\n",
        "\n",
        "    def summarise_with_openai(self, text: str) -> str:\n",
        "        # Call OpenAI to summarize the joined article texts\n",
        "        try:\n",
        "            response = self.openai.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert geopolitical analyst.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Summarise the following cluster of news in 150 words:\\n\\n{text}\"}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"(OpenAI error: {str(e)})\"\n",
        "\n",
        "    def run(self, df_clustered: pd.DataFrame) -> Dict[int, str]:\n",
        "        summaries = {}\n",
        "        for c in df_clustered.cluster.unique():\n",
        "            texts = df_clustered[df_clustered.cluster == c].text.tolist()\n",
        "            joined = \"\\n\".join(texts)[:6000]  # Truncate input\n",
        "            summary = self.summarise_with_openai(joined)\n",
        "            summaries[c] = summary\n",
        "        return summaries\n",
        "\n",
        "\n",
        "class GradioDashboardAgent:\n",
        "    \"\"\"Interactive Gradio dashboard for exploring article clusters.\"\"\"\n",
        "\n",
        "    def __init__(self, df_clustered, embedding_2d, summaries, model=\"gpt-4o-mini\"):\n",
        "        self.df = df_clustered.copy()\n",
        "        self.df[\"x\"] = embedding_2d[:, 0]\n",
        "        self.df[\"y\"] = embedding_2d[:, 1]\n",
        "        self.summaries = summaries\n",
        "\n",
        "        self.openai = openai\n",
        "        self.openai.api_key = openai.api_key\n",
        "        self.model = model\n",
        "\n",
        "    def generate_weekly_summary_txt(self):\n",
        "        # Generate weekly summary from recent articles using LLM\n",
        "        now = pd.Timestamp.utcnow()\n",
        "        week_ago = now - pd.Timedelta(days=7)\n",
        "\n",
        "        published_dates = pd.to_datetime(self.df[\"published\"], errors=\"coerce\", utc=True)\n",
        "        recent_articles = self.df[published_dates >= week_ago][\"text\"].dropna().tolist()\n",
        "\n",
        "        if not recent_articles:\n",
        "            summary = \"No news articles were published in the last 7 days.\"\n",
        "        else:\n",
        "            joined_text = \"\\n\\n\".join(recent_articles)[:8000]\n",
        "\n",
        "            try:\n",
        "                response = self.openai.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert geopolitical analyst.\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"Write a policy summary based on this AI news coverage:\\n\\n{joined_text}\"}\n",
        "                    ],\n",
        "                    temperature=0.3,\n",
        "                )\n",
        "                summary = response.choices[0].message.content.strip()\n",
        "            except Exception as e:\n",
        "                summary = f\"(OpenAI error: {str(e)})\"\n",
        "\n",
        "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".txt\", mode=\"w\", encoding=\"utf-8\")\n",
        "        tmp.write(summary)\n",
        "        tmp.close()\n",
        "        return tmp.name\n",
        "\n",
        "    def launch(self, share=False):\n",
        "        df_plot = self.df\n",
        "\n",
        "        fig = px.scatter(\n",
        "            df_plot,\n",
        "            x=\"x\",\n",
        "            y=\"y\",\n",
        "            color=\"topic\",\n",
        "            hover_name=\"title\",\n",
        "            hover_data={\"x\": False, \"y\": False, \"published\": True, \"link\": False}\n",
        "        )\n",
        "\n",
        "        def get_cluster_info(cluster_id):\n",
        "            # Display summary and article list for a selected cluster\n",
        "            cluster_id = int(cluster_id)\n",
        "            summary = self.summaries.get(cluster_id, \"(No summary)\")\n",
        "            table = df_plot[df_plot[\"cluster\"] == cluster_id][[\"title\", \"published\", \"link\"]].copy()\n",
        "            table.columns = [\"Title\", \"Published\", \"Link\"]\n",
        "            return summary, table\n",
        "\n",
        "        def download_csv(cluster_id):\n",
        "            # Download articles in selected cluster as CSV\n",
        "            cluster_id = int(cluster_id)\n",
        "            table = df_plot[df_plot[\"cluster\"] == cluster_id][[\"title\", \"published\", \"link\"]].copy()\n",
        "            table.columns = [\"Title\", \"Published\", \"Link\"]\n",
        "            tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
        "            table.to_csv(tmp_file.name, index=False)\n",
        "            return tmp_file.name\n",
        "\n",
        "        cluster_options = df_plot[[\"cluster\", \"topic\"]].drop_duplicates().sort_values(\"topic\")\n",
        "        choices = [(row[\"topic\"], row[\"cluster\"]) for _, row in cluster_options.iterrows()]\n",
        "\n",
        "        with gr.Blocks() as app:\n",
        "            with gr.Row():\n",
        "                gr.Markdown(\"<h2 style='text-align: center; font-weight: bold;'>üß≠ AI-Risk Radar ‚Äî Cluster Explorer</h2>\")\n",
        "\n",
        "            with gr.Row():\n",
        "                plot = gr.Plot(value=fig, label=\"Cluster Map\")\n",
        "\n",
        "            with gr.Row():\n",
        "                dropdown = gr.Dropdown(choices=choices, label=\"üéØ Select Topic\", interactive=True)\n",
        "\n",
        "            with gr.Row():\n",
        "                button = gr.Button(\"üîç Show Cluster Info\")\n",
        "\n",
        "            summary = gr.Textbox(label=\"üìù Cluster Summary\", lines=5, interactive=False)\n",
        "            table = gr.DataFrame(label=\"üì∞ Articles\", interactive=False)\n",
        "\n",
        "            with gr.Row():\n",
        "                download_btn = gr.Button(\"üóûÔ∏è Download News Articles (CSV)\")\n",
        "                weekly_report_btn = gr.Button(\"üìÖ Generate Weekly Report (.txt)\")\n",
        "\n",
        "            with gr.Row():\n",
        "                download_file = gr.File(label=\"üìÑ Download Link\")\n",
        "                weekly_report_file = gr.File(label=\"üóÇÔ∏è Weekly Report Download\")\n",
        "\n",
        "            # Bind dashboard events\n",
        "            button.click(fn=get_cluster_info, inputs=dropdown, outputs=[summary, table])\n",
        "            download_btn.click(fn=download_csv, inputs=dropdown, outputs=download_file)\n",
        "            weekly_report_btn.click(fn=self.generate_weekly_summary_txt, inputs=[], outputs=weekly_report_file)\n",
        "\n",
        "        app.launch(share=share, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üöÄ Run AI-Risk Radar Pipeline\n",
        "\n",
        "This cell runs the full pipeline:\n",
        "\n",
        "- üì∞ Scrape news from RSS feeds  \n",
        "- üß† Embed articles (saved in ChromaDB)  \n",
        "- üß≠ Cluster and label topics with LLM  \n",
        "- üìù Summarize clusters via OpenAI  \n",
        "- üíª Launch interactive Gradio dashboard\n"
      ],
      "metadata": {
        "id": "rfzHdEKT0W9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üèÉ‚Äç Starting multi-agent pipeline‚Ä¶\")\n",
        "\n",
        "# Step 1: Scrape news articles from RSS feeds\n",
        "news_agent = NewsScraperAgent(RSS_FEEDS)\n",
        "df_articles = news_agent.run()\n",
        "print(f\"üì∞ Fetched {len(df_articles)} articles\")\n",
        "# Stop if no valid articles were scraped\n",
        "if df_articles.empty:\n",
        "    raise RuntimeError(\"‚ùå No valid articles were scraped. Exiting.\")\n",
        "\n",
        "# Step 2: Embed articles into vector representations and store in ChromaDB\n",
        "embed_agent = EmbeddingAgent(EMBED_MODEL_NAME, VECTOR_DB_DIR)\n",
        "embeddings_df = embed_agent.run(df_articles)\n",
        "# Stop if embeddings failed\n",
        "if embeddings_df.empty or embeddings_df.shape[0] == 0:\n",
        "    raise RuntimeError(\"‚ùå Embedding failed. Check input text and embedding process.\")\n",
        "print(\"‚úÖ Embedding complete\")\n",
        "\n",
        "# Step 3: Cluster embedded articles and label each cluster using LLM\n",
        "cluster_agent = ClusteringAgent()\n",
        "df_clustered, emb2d, cluster_labels = cluster_agent.run(embeddings_df, df_articles)\n",
        "print(\"üè∑Ô∏è Cluster labels:\", cluster_labels)\n",
        "\n",
        "# Step 4: Summarize each cluster using OpenAI (LLM)\n",
        "summariser = SummariserAgent(model=\"gpt-4o-mini\")  # You can switch to \"gpt-3.5-turbo\" if needed\n",
        "cluster_summaries = summariser.run(df_clustered)\n",
        "print(\"üìù Summarisation complete\")\n",
        "\n",
        "# Step 5: Launch the interactive Gradio dashboard\n",
        "gradio_agent = GradioDashboardAgent(df_clustered, emb2d, cluster_summaries)\n",
        "print(\"üöÄ Launching Gradio app...\")\n",
        "gradio_agent.launch(share=True)  # Set to False to run locally without public link"
      ],
      "metadata": {
        "id": "C960nalA0ZXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}